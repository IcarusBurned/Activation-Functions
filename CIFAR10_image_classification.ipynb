{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_image_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IcarusBurned/Activation-Functions/blob/master/CIFAR10_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "3MaoI1agsIfz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image Classification with CIFAR-10 dataset\n",
        "In this notebook, I am going to classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded. Some more interesting datasets can be found [here](http://rodrigob.github.io/are_we_there_yet/build/#datasets)\n",
        "\n",
        "Some of the code and description of this notebook is borrowed by [this repo](https://github.com/udacity/deep-learning/tree/master/image-classification) provided by Udacity's Deep Learning Nanodegree program. This notebook has been reproduced decorated with richer descriptions after completing the Udacity's project.\n",
        "\n",
        "## Get the Data\n",
        "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
      ]
    },
    {
      "metadata": {
        "id": "LhwKAfFIsIf0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Some references to look up\n",
        "- [CIFAR-10/CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html): labeled subsets of the 80 million tiny images dataset. \n",
        "- [urlretrieve lib](https://docs.python.org/3.0/library/urllib.request.html): copy a network object denoted by a URL to a local file, if necessary\n",
        "- [tarfile lib](https://docs.python.org/2/library/tarfile.html): makes it possible to read and write tar archives, including those using gzip or bz2 compression.\n",
        "- [tqdm lib](https://pypi.python.org/pypi/tqdm): Fast, Extensible Progress Meter"
      ]
    },
    {
      "metadata": {
        "id": "shhxoMHcsIf2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "from os.path import isfile, isdir\n",
        "from tqdm import tqdm \n",
        "import tarfile\n",
        "\n",
        "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
        "\n",
        "class DownloadProgress(tqdm):\n",
        "    last_block = 0\n",
        "\n",
        "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
        "        self.total = total_size\n",
        "        self.update((block_num - self.last_block) * block_size)\n",
        "        self.last_block = block_num\n",
        "\n",
        "\"\"\" \n",
        "    check if the data (zip) file is already downloaded\n",
        "    if not, download it from \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" and save as cifar-10-python.tar.gz\n",
        "\"\"\"\n",
        "if not isfile('cifar-10-python.tar.gz'):\n",
        "    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
        "        urlretrieve(\n",
        "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
        "            'cifar-10-python.tar.gz',\n",
        "            pbar.hook)\n",
        "\n",
        "if not isdir(cifar10_dataset_folder_path):\n",
        "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
        "        tar.extractall()\n",
        "        tar.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aun44r70sIf4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Before Jumping in\n",
        "\n",
        "### Some references to look up\n",
        "- [python pickle](https://docs.python.org/3/library/pickle.html): implements binary protocols for serializing and de-serializing a Python object structure.\n",
        "- [numpy reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html): Gives a new shape to an array without changing its data.\n",
        "- [numpy transpose](https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html): Permute the dimensions of an array.\n",
        "- [numpy transpose with list of axes explanation](https://stackoverflow.com/questions/32034237/how-does-numpys-transpose-method-permute-the-axes-of-an-array)\n",
        "- [tensorflow conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d): check out input with the argument, data_format. \"NHWC\": [batch, height, width, channels], \"NCHW\": [batch, channels, height, width].\n",
        "- [row major order explanation](https://en.wikipedia.org/wiki/Row-_and_column-major_order)"
      ]
    },
    {
      "metadata": {
        "id": "B6KvLnD4sIf5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hjTq68jusIf7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### List of files \n",
        "\n",
        "![](./list_of_batch_files.png)\n",
        "\n",
        "As seen above picture, the dataset is broken into batches to **prevent** your machine from running **out of memory**. The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc..\n",
        "\n",
        "\n",
        "### Understanding the original data \n",
        "\n",
        "The original a batch data is (10000 x 3072) dimensional tensor expressed in numpy array, where the number of columns, (10000), indicates the number of sample data. As stated in the [CIFAR-10/CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), the row vector, (3072) represents an color image of 32x32 pixels. Since this project is going to use CNN for the classification tasks, the row vector, (3072), is not an appropriate form of image data to feed. In order to feed an image data into a CNN model, the dimension of the tensor representing an image data should be either (width x height x num_channel) or (num_channel x width x height). It depends on your choice (check out the [tensorflow conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)). In this particular project, I am going to use the dimension of the first choice because the default choice in tensorflow's CNN operation is so.\n",
        "\n",
        "[O] need to be modified into a new shape\n",
        "\n",
        "### Understanding the original labels\n",
        "\n",
        "The label data is just a list of 10000 numbers in the range 0-9, which corresponds to each of the 10 classes in CIFAR-10. \n",
        "\n",
        "* **airplane**\n",
        "* **automobile**\n",
        "* **bird**\n",
        "* **cat**\n",
        "* **deer**\n",
        "* **dog**\n",
        "* **frog**\n",
        "* **horse**\n",
        "* **ship**\n",
        "* **truck**\n",
        "\n",
        "[X] need to be modified into a new shape\n"
      ]
    },
    {
      "metadata": {
        "id": "6qouZ9p1sIf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_label_names():\n",
        "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ESHFUMTksIf-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### How to reshape into a such form?\n",
        "\n",
        "The row vector (3072) has the exact same number of elements if you calculate 32\\*32\\*3==3072. In order to reshape the row vector, (3072), there are two steps required. The **first** step is involved with using **reshape** function in numpy, and the **second** step is involved with using **transpose** function in numpy as well.\n",
        "\n",
        "By definition from the official web site, **reshape** function gives a new shape to an array without changing its data. Here, the phrase **without changing its data** is an important part. **reshape** operations should be delivered in three more detailed step. The following direction is described in a logical concept. \n",
        "\n",
        "1. divide the row vector (3072) into 3 pieces. Each piece corresponds to the each channels.\n",
        "  - this results in (3 x 1024) dimension of tensor\n",
        "2. divide the resulting tensor from the previous step with 32. 32 here means width of an image.\n",
        "  - this results in (3 x 32 x 32)\n",
        "\n",
        "In order to implement the directions written in logical sense in numpy, **reshape** function should be called in the following arguments, (10000, 3, 32, 32). As you noticed, reshape function doesn't automatically divide further when the third value (32, width) is provided. We need to explicitly specify the value for the last value (32, height)\n",
        "\n",
        "\n",
        "This is not the end of story. Now, the image data is represented as (num_channel, width, height) form. However, **this is not the shape tensorflow and matplotlib are expecting**. They are expecting different shape of (width, height, num_channel) instead. We need to swap the order of each axes, and that is where **transpose** function comes in.\n",
        "\n",
        "The **transpose** function can take a list of axes, and each value specifies where it wants to move around. For example, calling transpose with argument (1, 2, 0) in an numpy array of (num_channel, width, height) will return a new numpy array of (width, height, num_channel).\n",
        "\n",
        "<img src=\"./reshape-transpose.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
      ]
    },
    {
      "metadata": {
        "id": "K3WP1gHFsIf_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
        "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
        "        # note the encoding type is 'latin1'\n",
        "        batch = pickle.load(file, encoding='latin1')\n",
        "        \n",
        "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "    labels = batch['labels']\n",
        "        \n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iVzwvowRsIgC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Explore the Data\n",
        "\n",
        "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
        "\n",
        "The display_stats defined below answers some of questions like in a given batch of data..\n",
        "- \"What are all possible labels?\"\n",
        "- \"What is the range of values for the image data?\"\n",
        "- \"Are the labels in order or random?\"\n"
      ]
    },
    {
      "metadata": {
        "id": "uGtejDrssIgC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
        "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
        "    \n",
        "    if not (0 <= sample_id < len(features)):\n",
        "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
        "        return None\n",
        "\n",
        "    print('\\nStats of batch #{}:'.format(batch_id))\n",
        "    print('# of Samples: {}\\n'.format(len(features)))\n",
        "    \n",
        "    label_names = load_label_names()\n",
        "    label_counts = dict(zip(*np.unique(labels, return_counts=True)))\n",
        "    for key, value in label_counts.items():\n",
        "        print('Label Counts of [{}]({}) : {}'.format(key, label_names[key].upper(), value))\n",
        "    \n",
        "    sample_image = features[sample_id]\n",
        "    sample_label = labels[sample_id]\n",
        "    \n",
        "    print('\\nExample of Image {}:'.format(sample_id))\n",
        "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
        "    print('Image - Shape: {}'.format(sample_image.shape))\n",
        "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
        "    \n",
        "    plt.imshow(sample_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G8DRscjWsIgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "a64c994b-b962-4b1e-9b65-4b261a582786"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Explore the dataset\n",
        "batch_id = 3\n",
        "sample_id = 7000\n",
        "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Stats of batch #3:\n",
            "# of Samples: 10000\n",
            "\n",
            "Label Counts of [0](AIRPLANE) : 994\n",
            "Label Counts of [1](AUTOMOBILE) : 1042\n",
            "Label Counts of [2](BIRD) : 965\n",
            "Label Counts of [3](CAT) : 997\n",
            "Label Counts of [4](DEER) : 990\n",
            "Label Counts of [5](DOG) : 1029\n",
            "Label Counts of [6](FROG) : 978\n",
            "Label Counts of [7](HORSE) : 1015\n",
            "Label Counts of [8](SHIP) : 961\n",
            "Label Counts of [9](TRUCK) : 1029\n",
            "\n",
            "Example of Image 7000:\n",
            "Image - Min Value: 24 Max Value: 252\n",
            "Image - Shape: (32, 32, 3)\n",
            "Label - Label Id: 0 Name: airplane\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAHxCAYAAAB5x1VAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJUnK3P8HDVTlK+4umqsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 249,
              "height": 248
            }
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XsWOZGJ5sIgJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implement Preprocess Functions\n",
        "### Normalize\n",
        "**Min-Max Normalization**\n",
        "- this simply makes all x values to range between 0 and 1.\n",
        "- y = (x-min) / (max-min)\n",
        "\n",
        "**Some references to look up**\n",
        "- [Min-Max Normalization](https://www.quora.com/What-is-the-meaning-of-min-max-normalization)\n",
        "- [Watch \"why normalizing inputs\" / deeplearning.ai - Andrew Ng.](https://www.youtube.com/watch?v=FDCfw-YqWTE)\n",
        "- [Exploding, Vainishing Gradient descent / deeplearning.ai - Andrew Ng.](https://www.youtube.com/watch?v=qhXZsFVxGKo)\n",
        "\n",
        "`normalize` function takes an image data, `x`, and returns it as a normalized Numpy array. The values in the original data is going to be transformed in range of 0 to 1, inclusive without change the shape of the array. A simply answer to why normalization should be performed is somewhat related to activation function.\n",
        "\n",
        "For example, sigmoid activation function takes an input value and outputs a new value ranging from 0 to 1. When the input value is somewhat large, the output value easily reaches the max value 1. Similarily, when the input value is somewhat small, the output value easily reaches the max value 0. \n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
        "\n",
        "For another example, ReLU activation function takes an input value and outputs a new value ranging from 0 to infinity. When the input value is somewhat large, the output value increases linearly. However, when the input value is somewhat small, the output value easily reaches the max value 0. \n",
        "\n",
        "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_4/Relu.jpeg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
        "\n",
        "Now, when we think about the image data, all values originally ranges from 0 to 255. This sounds like when it is passed into sigmoid function, the output is almost always 1, and when it is passed into ReLu function, the output could be very huge. When backpropagation process is performed to optimize the networks, this could lead to an exploding gradient which leads to an aweful learning steps. In order to avoid this issue, ideally, it is better let all the values be around 0 and 1."
      ]
    },
    {
      "metadata": {
        "id": "yzG25W5EsIgK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "    \"\"\"\n",
        "        argument\n",
        "            - x: input image data in numpy array [32, 32, 3]\n",
        "        return\n",
        "            - normalized x \n",
        "    \"\"\"\n",
        "    min_val = np.min(x)\n",
        "    max_val = np.max(x)\n",
        "    x = (x-min_val) / (max_val-min_val)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-fKHn-NhsIgM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### One-hot encode\n",
        "\n",
        "Since the output of our model is going to show the probabilities of where an image should be categorized as a prediction. There should be a vector having the same number of elements as the number of image classes. For instance, CIFAR-10 provides 10 different classes of image, so we need a vector in size of 10 as well. Each element represents the predicting probability of each classes.\n",
        "\n",
        "Also, our model should be able to compare the prediction with the ground truth label. It means the shape of the label data should also be transformed into a vector in size of 10 too. Instead, because label is the ground truth, we set the value 1 to the corresponding element.\n",
        "\n",
        "**`one_hot_encode`** function takes the input, **`x`**, which is a list of labels(ground truth). The total number of element in the list is the total number of samples in a batch. **`one_hot_encode`** function returns a 2 dimensional tensor, where the number of row is the size of the batch, and the number of column is the number of image classes.\n",
        "\n",
        "#### some references to look up\n",
        "- [one hot encoding](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science)"
      ]
    },
    {
      "metadata": {
        "id": "OO-SQPDusIgN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_encode(x):\n",
        "    \"\"\"\n",
        "        argument\n",
        "            - x: a list of labels\n",
        "        return\n",
        "            - one hot encoding matrix (number of labels, number of class)\n",
        "    \"\"\"\n",
        "    encoded = np.zeros((len(x), 10))\n",
        "    \n",
        "    for idx, val in enumerate(x):\n",
        "        encoded[idx][val] = 1\n",
        "    \n",
        "    return encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6jX9dRiksIgQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocess all the data and save it\n",
        "\n",
        "The code cell below uses the previously implemented functions, normalize and one_hot_encode, to preprocess the given dataset.\n",
        "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation. \n",
        "\n",
        "<img src=\"./train-valid-test split.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
      ]
    },
    {
      "metadata": {
        "id": "AvbnPoQ-v1hG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "927fc0c8-79c9-4959-fe3a-25b097446dd6"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zc62ejUNsIgR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
        "    features = normalize(features)\n",
        "    labels = one_hot_encode(labels)\n",
        "\n",
        "    pickle.dump((features, labels), open(filename, 'wb'))\n",
        "\n",
        "\n",
        "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
        "    n_batches = 5\n",
        "    valid_features = []\n",
        "    valid_labels = []\n",
        "\n",
        "    for batch_i in range(1, n_batches + 1):\n",
        "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
        "        \n",
        "        # find index to be the point as validation data in the whole dataset of the batch (10%)\n",
        "        index_of_validation = int(len(features) * 0.1)\n",
        "\n",
        "        # preprocess the 90% of the whole dataset of the batch\n",
        "        # - normalize the features\n",
        "        # - one_hot_encode the lables\n",
        "        # - save in a new file named, \"preprocess_batch_\" + batch_number\n",
        "        # - each file for each batch\n",
        "        _preprocess_and_save(normalize, one_hot_encode,\n",
        "                             features[:-index_of_validation], labels[:-index_of_validation], \n",
        "                             'preprocess_batch_' + str(batch_i) + '.p')\n",
        "\n",
        "        # unlike the training dataset, validation dataset will be added through all batch dataset\n",
        "        # - take 10% of the whold dataset of the batch\n",
        "        # - add them into a list of\n",
        "        #   - valid_features\n",
        "        #   - valid_labels\n",
        "        valid_features.extend(features[-index_of_validation:])\n",
        "        valid_labels.extend(labels[-index_of_validation:])\n",
        "\n",
        "    # preprocess the all stacked validation dataset\n",
        "    _preprocess_and_save(normalize, one_hot_encode,\n",
        "                         np.array(valid_features), np.array(valid_labels),\n",
        "                         'preprocess_validation.p')\n",
        "\n",
        "    # load the test dataset\n",
        "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding='latin1')\n",
        "\n",
        "    # preprocess the testing data\n",
        "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "    test_labels = batch['labels']\n",
        "\n",
        "    # Preprocess and Save all testing data\n",
        "    _preprocess_and_save(normalize, one_hot_encode,\n",
        "                         np.array(test_features), np.array(test_labels),\n",
        "                         'preprocess_training.p')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HY3fExaRsIgU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oz6jPShZsIgY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Checkpoint\n"
      ]
    },
    {
      "metadata": {
        "id": "jTLJOW5PsIgZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPjc8zUNsIgc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Basic\n",
        "\n",
        "Before diving in building the network and training process, it is good to remind myself how Tensorflow works. \n",
        "\n",
        "### Tensorflow Packages\n",
        "Tensorflow comes with bunch of packages. You can even find modules having similary functionalities. For instance, tf.nn.conv2d and tf.layers.conv2d are both 2-D convolutional operations. Here are the purposes of the categories of each packages.\n",
        "\n",
        "#### tf.nn: lower level APIs for neural network\n",
        "  - each APIs under this package has its sole purpose\n",
        "  - for instance, in order to apply activation function after conv2d, you need two separate API calls\n",
        "  - you probably have to set lots of settings by yourself manually\n",
        "\n",
        "#### tf.layers: higher level APIs for neural network\n",
        "  - each APIs under this package probably has streamlined processes\n",
        "  - for instance, in order to apply activation function after conv2d, you don't need two spearate API calls. Intead, conv2d API under this package has activation argument\n",
        "  - each APIs under this package comes with lots of default setting in arguments\n",
        "  \n",
        "#### tf.contrib: contrib module containing volatile or experimental code\n",
        "  - like the documents explain, this package provides experimental codes\n",
        "  - you could probably find more handy APIs\n",
        "  - you could look up this package when you don't find functionality under the main packages\n",
        "  - It is meant to contain features and contributions that eventually should get merged into core TensorFlow, but you can think of them like under construction\n",
        "\n",
        "I am going to use APIs under each different packages so that I could be familiar with different API usages. Below is the list of references APIs I am going to use for implementing the entire model.\n",
        "\n",
        "#### some references to look up\n",
        "\n",
        "- [Stanford CS20 'Tensorflow for Deep Learning Research'](https://web.stanford.edu/class/cs20si/syllabus.html)\n",
        "\n",
        "### Tensorflow Workflow\n",
        "\n",
        "According to the official document, **TensorFlow** uses a **dataflow graph** to represent your computation in terms of the dependencies between individual operations. This leads to a low-level programming model in which you **first define the dataflow graph**, **then** create a TensorFlow **session to run parts of the graph** across a set of local and remote devices.\n",
        "\n",
        "**Dataflow** is a common programming model for **parallel computing**. In a dataflow graph, the nodes represent units of computation, and the edges represent the data consumed or produced by a computation. For example, in a TensorFlow graph, the tf.matmul operation would correspond to a single node with two incoming edges (the matrices to be multiplied) and one outgoing edge (the result of the multiplication).\n",
        "\n",
        "Most **TensorFlow** programs **start with a dataflow graph construction phase**. In this phase, you invoke TensorFlow API functions that construct new tf.Operation (node) and tf.Tensor (edge) objects and add them to a tf.Graph instance. **TensorFlow provides a default graph** that is an **implicit argument to all API functions in the same context**.\n",
        "\n",
        "The **tf.Session.run method** is the main mechanism for **running a tf.Operation or evaluating a tf.Tensor**. You can pass one or more tf.Operation or tf.Tensor objects to tf.Session.run, and TensorFlow will execute the operations that are needed to compute the result.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/versions/r1.3/images/tensors_flowing.gif\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
        "\n",
        "#### some references to look up\n",
        "- [Tensorflow Architecture](https://www.tensorflow.org/extend/architecture)\n",
        "- [Tensorflow Graphs and Sessions](https://www.tensorflow.org/versions/r1.3/programmers_guide/graphs)"
      ]
    },
    {
      "metadata": {
        "id": "pvRPS4nlsIgd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the network\n",
        "\n",
        "The image below depicts what the model would look like in this notebook.\n",
        "\n",
        "<img src=\"./conv_model.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
      ]
    },
    {
      "metadata": {
        "id": "YWT8S4Q_sIgd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare Input for the Model\n",
        "\n",
        "#### some references to look up\n",
        "- [Tensorflow Data Type](https://www.tensorflow.org/api_docs/python/tf/DType)\n",
        "- [Tensorflow Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
        "- [Tensorflow Variable](https://www.tensorflow.org/api_docs/python/tf/Variable)"
      ]
    },
    {
      "metadata": {
        "id": "wVEh7RQesIge",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Remove previous weights, bias, inputs, etc..\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Inputs\n",
        "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
        "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
        "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "peEo2DVssIgh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create Convolutional Model\n",
        "\n",
        "The entire model consists of 14 layers in total. In addition to layers below lists what techniques are applied to build the model.\n",
        "\n",
        "1. Convolution with 64 different filters in size of (3x3)\n",
        "2. Max Pooling by 2\n",
        "  - ReLU activation function \n",
        "  - Batch Normalization\n",
        "3. Convolution with 128 different filters in size of (3x3)\n",
        "4. Max Pooling by 2\n",
        "  - ReLU activation function \n",
        "  - Batch Normalization\n",
        "5. Convolution with 256 different filters in size of (3x3)\n",
        "6. Max Pooling by 2\n",
        "  - ReLU activation function \n",
        "  - Batch Normalization\n",
        "7. Convolution with 512 different filters in size of (3x3)\n",
        "8. Max Pooling by 2\n",
        "  - ReLU activation function \n",
        "  - Batch Normalization\n",
        "9. Flattening the 3-D output of the last convolutional operations.\n",
        "10. Fully Connected Layer with 128 units\n",
        "  - Dropout \n",
        "  - Batch Normalization\n",
        "11. Fully Connected Layer with 256 units\n",
        "  - Dropout \n",
        "  - Batch Normalization\n",
        "12. Fully Connected Layer with 512 units\n",
        "  - Dropout \n",
        "  - Batch Normalization\n",
        "13. Fully Connected Layer with 1024 units\n",
        "  - Dropout \n",
        "  - Batch Normalization\n",
        "14. Fully Connected Layer with 10 units (number of image classes)\n",
        "\n",
        "the image below decribes how the conceptual convolving operation differs from the tensorflow implementation when you use [Channel x Width x Height] tensor format. \n",
        "\n",
        "<img src=\"./convolving.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
        "\n",
        "#### some references to look up\n",
        "- [Tensorflow Conv2D under tf.nn](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
        "- [Tensorflow ReLU under tf.nn](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
        "- [Tensorflow Max Pooling under tf.nn](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool)\n",
        "- [Tensorflow Dropout under tf.nn](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)\n",
        "- [Tensorflow Batch Normalization under tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization)\n",
        "- [Tensorflow Flatten under tf.contrib](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten)\n",
        "- [Tensorflow Fully Connected under tf.contrib](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected)\n",
        "- [Batch Normalization (the original paper)](https://arxiv.org/abs/1502.03167)\n",
        "- [Why does Batch Norm works? / deeplearning.ai - Andrew Ng.](https://www.youtube.com/watch?v=NbGUU6ZYtus)\n",
        "- [Dropout (the original paper)](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\n",
        "- [Understanding Dropout / deeplearning.ai - Andrew Ng.](https://www.youtube.com/watch?v=ARq74QuavAo)\n",
        "- [Dropout in (Deep) Machine learning](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)\n",
        "- [What is the meaning of flattening step in a convolutional neural network?](https://www.quora.com/What-is-the-meaning-of-flattening-step-in-a-convolutional-neural-network)\n",
        "- [Convolutional Neural Networks (CNNs / ConvNets) - CS231n](http://cs231n.github.io/convolutional-networks/)\n",
        "- [Visualizing and Understanding Convolutional Networks](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)\n",
        "- [Evaluation of the CNN design choices performance on ImageNet-2012](https://github.com/ducha-aiki/caffenet-benchmark)"
      ]
    },
    {
      "metadata": {
        "id": "UZ36OOJ7tP4J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "PIx2 = 2.0*tf.constant(math.pi)\n",
        "PI = tf.constant(math.pi)\n",
        "\n",
        "def ReLU(x):\n",
        "  return tf.maximum(0.0, x)\n",
        "\n",
        "def xsinxRelu(x):\n",
        "  return tf.maximum(0.0,1.0/PIx2*(PIx2*x - tf.sin(PIx2*x)))\n",
        "\n",
        "def expxsinxRelu(x):\n",
        "  s = 1.0\n",
        "  return tf.maximum(0.0,1.0/PIx2*(PIx2*x - tf.exp(-s*tf.square(x))* tf.sin(PIx2*x)))\n",
        "\n",
        "def xsinxSigmoid(x):\n",
        "  y = tf.minimum(1.0, tf.maximum(0.0,1.0/(PIx2)*((x + PI) - tf.sin((x + PI)))))\n",
        "  return y / tf.reduce_sum(y)\n",
        "\n",
        "def lisht(x):\n",
        "  return x*(tf.exp(x) - tf.exp(-x))/(tf.exp(x) + tf.exp(-x))\n",
        "\n",
        "def swish(x):\n",
        "  return x*tf.exp(x)/(tf.exp(x) + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fR9-Vqz1sIgi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def conv_net(x, keep_prob):\n",
        "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
        "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
        "    conv3_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
        "    conv4_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))\n",
        "\n",
        "    # 1, 2\n",
        "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
        "    #conv1 = tf.nn.relu(conv1)\n",
        "    conv1 = expxsinxRelu(conv1)\n",
        "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
        "\n",
        "    # 3, 4\n",
        "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
        "    #conv2 = tf.nn.relu(conv2)\n",
        "    conv2 = expxsinxRelu(conv2)\n",
        "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
        "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
        "  \n",
        "    # 5, 6\n",
        "    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
        "    #conv3 = tf.nn.relu(conv3)\n",
        "    conv3 = expxsinxRelu(conv3)\n",
        "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
        "    conv3_bn = tf.layers.batch_normalization(conv3_pool)\n",
        "    \n",
        "    # 7, 8\n",
        "    conv4 = tf.nn.conv2d(conv3_bn, conv4_filter, strides=[1,1,1,1], padding='SAME')\n",
        "    #conv4 = tf.nn.relu(conv4)\n",
        "    conv4 = expxsinxRelu(conv4)\n",
        "    conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "    conv4_bn = tf.layers.batch_normalization(conv4_pool)\n",
        "    \n",
        "    # 9\n",
        "    flat = tf.contrib.layers.flatten(conv4_bn)  \n",
        "\n",
        "    # 10\n",
        "    #full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
        "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=expxsinxRelu)\n",
        "    full1 = tf.nn.dropout(full1, keep_prob)\n",
        "    full1 = tf.layers.batch_normalization(full1)\n",
        "    \n",
        "    # 11\n",
        "    #full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
        "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=expxsinxRelu)\n",
        "    full2 = tf.nn.dropout(full2, keep_prob)\n",
        "    full2 = tf.layers.batch_normalization(full2)\n",
        "    \n",
        "    # 12\n",
        "    #full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.relu)\n",
        "    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=expxsinxRelu)\n",
        "    full3 = tf.nn.dropout(full3, keep_prob)\n",
        "    full3 = tf.layers.batch_normalization(full3)    \n",
        "    \n",
        "    # 13\n",
        "    #full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
        "    full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=expxsinxRelu)\n",
        "    full4 = tf.nn.dropout(full4, keep_prob)\n",
        "    full4 = tf.layers.batch_normalization(full4)        \n",
        "    \n",
        "    # 14\n",
        "    out = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=10, activation_fn=None)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2JWQTp4PsIgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters\n",
        "\n",
        "* `epochs`: number of iterations until the network stops learning or start overfitting\n",
        "* `batch_size`: highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
        "* `keep_probability`: probability of keeping a node using dropout\n",
        "* `learning_rate`: number how fast the model learns"
      ]
    },
    {
      "metadata": {
        "id": "JAoYa-nlsIgr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "keep_probability = 0.7\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7HObJhYPsIgt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Cost Function & Optimizer\n",
        "\n",
        "- [Tensorflow Softmax Cross Entropy with Logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2)\n",
        "- [Tensorflow Reduce Mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)\n",
        "- [Tensorflow Optimizers](https://www.tensorflow.org/api_guides/python/train)\n",
        "- [Tensorflow Equal](https://www.tensorflow.org/api_docs/python/tf/equal)\n",
        "- [Tensorflow Cast](https://www.tensorflow.org/api_docs/python/tf/cast)\n",
        "- [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)\n",
        "- [Optimization for Training Deep Models](http://www.deeplearningbook.org/contents/optimization.html)"
      ]
    },
    {
      "metadata": {
        "id": "jDQavG2msIgu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = conv_net(x, keep_prob)\n",
        "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
        "\n",
        "# Loss and Optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Accuracy\n",
        "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nEwAihafsIgw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the Neural Network\n",
        "\n",
        "We have defined cost, optimizer and accuracy, and what they really are is..\n",
        "- **cost**:  \n",
        "   - tf.reduce_mean returns => The reduced **Tensor**\n",
        "- **optimizer**:  \n",
        "   - tf.train.AdamOptimizer returns => An **Operation** that applies the specified gradients.\n",
        "- **accuracy**: \n",
        "   - tf.reduce_mean returns => The reduced **Tensor**\n",
        "\n",
        "tf.Session.run method in the official document explains it runs one \"step\" of TensorFlow computation, by running the necessary graph fragment to execute every Operation and evaluate every Tensor in fetches, substituting the values in feed_dict for the corresponding input values. The fetches argument may be a single graph element, or an arbitrarily nested list, tuple, etc.\n",
        "\n",
        "Here what graph element really is tf.Tensor or tf.Operation. Cost, Optimizer, and Accuracy are one of those types meaning they can be specified as part of the fetches argument. Then, we can feed some variables along the way. This is kind of handy feature of Tensorflow. Once we have constructed the graph, all we need to do is feeding data into that graph and specifying what results to retrieve.\n",
        "\n",
        "\n",
        "\n",
        "#### some references to look up\n",
        "- [Tensorflow Session run function](https://www.tensorflow.org/api_docs/python/tf/Session#run)\n",
        "- [Tensorflow tf.reduce_mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)\n",
        "- [Tensorflow tf.train.AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)\n",
        "\n",
        "### Single Optimization\n"
      ]
    },
    {
      "metadata": {
        "id": "stIHL3yAsIgx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
        "    session.run(optimizer, \n",
        "                feed_dict={\n",
        "                    x: feature_batch,\n",
        "                    y: label_batch,\n",
        "                    keep_prob: keep_probability\n",
        "                })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hd8g4mUgsIg1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Show Stats\n"
      ]
    },
    {
      "metadata": {
        "id": "p_QcFsRIsIg2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
        "    loss = sess.run(cost, \n",
        "                    feed_dict={\n",
        "                        x: feature_batch,\n",
        "                        y: label_batch,\n",
        "                        keep_prob: 1.\n",
        "                    })\n",
        "    valid_acc = sess.run(accuracy, \n",
        "                         feed_dict={\n",
        "                             x: valid_features,\n",
        "                             y: valid_labels,\n",
        "                             keep_prob: 1.\n",
        "                         })\n",
        "    \n",
        "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zupDh-tsIg5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Fully Train the Model"
      ]
    },
    {
      "metadata": {
        "id": "BBAK1gVasIg6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_features_labels(features, labels, batch_size):\n",
        "    \"\"\"\n",
        "    Split features and labels into batches\n",
        "    \"\"\"\n",
        "    for start in range(0, len(features), batch_size):\n",
        "        end = min(start + batch_size, len(features))\n",
        "        yield features[start:end], labels[start:end]\n",
        "\n",
        "def load_preprocess_training_batch(batch_id, batch_size):\n",
        "    \"\"\"\n",
        "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
        "    \"\"\"\n",
        "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
        "    features, labels = pickle.load(open(filename, mode='rb'))\n",
        "\n",
        "    # Return the training data in batches of size <batch_size> or less\n",
        "    return batch_features_labels(features, labels, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SgR1qJersIg8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "37416a58-e96f-41c9-fec9-1e17d50191e7"
      },
      "cell_type": "code",
      "source": [
        "save_model_path = './image_classification'\n",
        "\n",
        "print('Training...')\n",
        "with tf.Session() as sess:\n",
        "    # Initializing the variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(epochs):\n",
        "        # Loop over all batches\n",
        "        n_batches = 5\n",
        "        for batch_i in range(1, n_batches + 1):\n",
        "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
        "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
        "                \n",
        "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
        "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
        "            \n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    save_path = saver.save(sess, save_model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2158 Validation Accuracy: 0.180600\n",
            "Epoch  1, CIFAR-10 Batch 2:  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gSluzRlGsIhA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Checkpoint\n",
        "The model has been saved to disk.\n",
        "## Test Model"
      ]
    },
    {
      "metadata": {
        "id": "G9MXi-PisIhB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "def batch_features_labels(features, labels, batch_size):\n",
        "    \"\"\"\n",
        "    Split features and labels into batches\n",
        "    \"\"\"\n",
        "    for start in range(0, len(features), batch_size):\n",
        "        end = min(start + batch_size, len(features))\n",
        "        yield features[start:end], labels[start:end]\n",
        "\n",
        "def display_image_predictions(features, labels, predictions, top_n_predictions):\n",
        "    n_classes = 10\n",
        "    label_names = load_label_names()\n",
        "    label_binarizer = LabelBinarizer()\n",
        "    label_binarizer.fit(range(n_classes))\n",
        "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
        "\n",
        "    fig, axies = plt.subplots(nrows=top_n_predictions, ncols=2, figsize=(20, 10))\n",
        "    fig.tight_layout()\n",
        "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
        "\n",
        "    n_predictions = 3\n",
        "    margin = 0.05\n",
        "    ind = np.arange(n_predictions)\n",
        "    width = (1. - 2. * margin) / n_predictions\n",
        "   \n",
        "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
        "        if (image_i < top_n_predictions):\n",
        "            pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
        "            correct_name = label_names[label_id]\n",
        "            \n",
        "            axies[image_i][0].imshow((feature*255).astype(np.int32, copy=False))\n",
        "            axies[image_i][0].set_title(correct_name)\n",
        "            axies[image_i][0].set_axis_off()\n",
        "\n",
        "            axies[image_i][1].barh(ind + margin, pred_values[:3], width)\n",
        "            axies[image_i][1].set_yticks(ind + margin)\n",
        "            axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
        "            axies[image_i][1].set_xticks([0, 0.5, 1.0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hDYZaapGsIhE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "save_model_path = './image_classification'\n",
        "batch_size = 64\n",
        "n_samples = 10\n",
        "top_n_predictions = 5\n",
        "\n",
        "def test_model():\n",
        "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
        "    loaded_graph = tf.Graph()\n",
        "\n",
        "    with tf.Session(graph=loaded_graph) as sess:\n",
        "        # Load model\n",
        "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
        "        loader.restore(sess, save_model_path)\n",
        "\n",
        "        # Get Tensors from loaded model\n",
        "        loaded_x = loaded_graph.get_tensor_by_name('input_x:0')\n",
        "        loaded_y = loaded_graph.get_tensor_by_name('output_y:0')\n",
        "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
        "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
        "        \n",
        "        # Get accuracy in batches for memory limitations\n",
        "        test_batch_acc_total = 0\n",
        "        test_batch_count = 0\n",
        "        \n",
        "        for train_feature_batch, train_label_batch in batch_features_labels(test_features, test_labels, batch_size):\n",
        "            test_batch_acc_total += sess.run(\n",
        "                loaded_acc,\n",
        "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
        "            test_batch_count += 1\n",
        "\n",
        "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
        "\n",
        "        # Print Random Samples\n",
        "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
        "        random_test_predictions = sess.run(\n",
        "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
        "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
        "        display_image_predictions(random_test_features, random_test_labels, random_test_predictions, top_n_predictions)\n",
        "\n",
        "\n",
        "test_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WTvU3ILWsIhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Other Models & their Accuracies \n",
        "[Classification datasets results - well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130)"
      ]
    },
    {
      "metadata": {
        "id": "o-Uo8QfWsIhJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}